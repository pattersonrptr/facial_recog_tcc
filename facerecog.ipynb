{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconhecimento Facial Aplicado a um Sistema de Login\n",
    "\n",
    "Este software tem o objetivo de solucionar o problema do login de usuário pela abordagem de reconhecimento facial.\n",
    "\n",
    "O Reconhecimento facial se depara comumente com dois problemas:\n",
    "\n",
    "- **Verificação Facial** - \"esta é a pessoa reinvindicada?\". Por exemplo, num sistema onde uma pessoa precisa usar um cartão de acesso em uma catraca, uma câmera apontada para o rosto da pessoa pode verificar se a pessoa que está usando o cartão é a pessoa correta (o dono do cartão de acesso). Um celular que desbloqueia usando seu rosto também está usando a verificação facial. Este é um problema de correspondência 1:1. \n",
    "- **Reconhecimento Facial** - \"quem é essa pessoa?\" Por exemplo, ao invés de usar um cartão de acesso, uma pessoa só precisaria ter sua face filmada por uma câmera, o sistema então compara a imagem captada com as imagens de rostos registradas na base de dados, se a pessoa existir na base, será reconhecida e poderá passar pela catraca. Este é um problema de correspondência de 1:K.\n",
    "\n",
    "Usamos uma Rede Neural que codifica a imagem de um rosto em um vetor de 128 números. Comparando tais vetores, pode-se determinar se duas imagens correspondem à mesma pessoa.\n",
    "    \n",
    "**No desenvolvimento desta aplicação foram abordados: **\n",
    "- A implementação da função de erro tripla\n",
    "- Foi usado um modelo pré-treinado de rede neural para mapear imagens em um vetor codificado de 128 posições\n",
    "- Essas codificações são então usadas para realizar a verificação facial e o reconhecimento facial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código abaixo carrega as bibliotecas necessárias para o projeto. A maioria são modulos do framework de Machine Learning Keras e bibliotecas úteis da linguagem Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D, AveragePooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.engine.topology import Layer\n",
    "from keras import backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from fr_utils import *\n",
    "from inception_blocks_v2 import *\n",
    "# Configuração necessária para o Jupyter exibir os gráficos gerados pela biblioteca matplotlib.\n",
    "%matplotlib inline    \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 - Verificação Facial Ingênua\n",
    "\n",
    "Na Verificação Facial, são dadas duas imagens e o sistema deve dizer se elas são da mesma pessoa. A forma mais fácil de fazer isso seria comparar as duas imagens pixel-por-pixel. Se a distância entre as imagens brutas for menor que um limite escolhido, pode ser a mesma pessoa!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "É claro que esta abordagem é muito pobre, visto que os valores dos pixels podem mudar bastante devido à variações na iluminação, orientação do rosto da pessoa, e até mesmo pequenas mudanças na posição da cabeça e etc...\n",
    "\n",
    "Por isso, melhor que usar imagens brutas, é possível aprender uma com uma função de codificação $f(img)$ para que as comparações de elementos dessa codificação forneçam avaliações mais precisas sobre se duas imagens são da mesma pessoa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Codificando imagens de rostos em um vetor de 128 posições\n",
    "\n",
    "### 1.1 - Usando uma ConvNet para calcular codificações\n",
    "\n",
    "O modelo de rede neural usa muitos dados e muito tempo para treinar. \n",
    "\n",
    "Como dissemos anteriormente, usamos um modelo de rede neural pré-treinado. Nós apenas precisamos carregar os parãmetros de peso treinados pelo modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mas é interessnate saber que:\n",
    "\n",
    "- Esta rede recebe imagens RGB de 96x96 dimensões em sua entrada. Especificamente, a entrada é um rosto (ou um conjunto de imagens de rostos) como um tensor de dimensões $(m, n_C, n_H, n_W) = (m, 3, 96, 96)$ onde 'm' é o número de exemplos de treino, o '3' corresponde às três camadas RGB e 96x96 é a dimensão das imagens.\n",
    "- Sua saída é uma matriz de $(m, 128)$ que codifica cada imagem de rosto de entrada em um vetor de 128 posições.\n",
    "\n",
    "O código a seguir cria uma instância do modelo para imagens de rostos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel = faceRecoModel(input_shape=(3, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade total de parâmetros: 3743280\n"
     ]
    }
   ],
   "source": [
    "print(\"Quantidade total de parâmetros:\", FRmodel.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando uma camada de 128 neurônios completamente conectados como sua última camada, o modelo garante que a saída será um vetor codificado de tamanho 128. Então essa codificação pode ser usada para comparar duas imagens de rostos.\n",
    "Para uma codificação ser boa é preciso que: \n",
    "- A codificações de duas imagens da mesma pessoa sejam muito parecidas uma com a outra. \n",
    "- As codificações de duas imagens de diferentes pessoas sejam muito diferentes.\n",
    "\n",
    "A função de erro tripla formaliza isso, e tenta \"empurrar\" as codificações das duas imagens da mesma pessoa (Âncora e Positiva) o mais pŕoximo possivel uma da outra, enquanto \"puxa\" as codificaçõe de imagens de diferentes pessoas (Âncora e Negativa) para mais lonje uma da outra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### 1.2 - A Função de Erro Tripla\n",
    "\n",
    "Para uma imagem $x$, denotamos sua codificação $f(x)$, onde $f$ é a função computada pela rede neural.\n",
    "\n",
    "\n",
    "O treino usa triplas de imagens $(A, P, N)$ onde:  \n",
    "\n",
    "- A é uma imagem \"Âncora\" -- a imagem de uma pessoa. \n",
    "- P é uma imagem \"Positiva\" -- uma imagem da mesma pessoa da imagem âncora.\n",
    "- N é uma imagem \"Negativa\" -- uma imagem de uma pessoa diferente da pessoa da imagem âncora.\n",
    "\n",
    "Essas triplas vêm do nosso conjunto de dados de treino. Nós escrevemos $(A^{(i)}, P^{(i)}, N^{(i)})$ para denotar o $i$-ésimo treino de exemplo. \n",
    "\n",
    "O Gradiente Descendente deve minimizar a seguinte função de erro tripla:\n",
    "\n",
    "$$\\mathcal{J} = \\sum^{m}_{i=1} \\large[ \\small \\underbrace{\\mid \\mid f(A^{(i)}) - f(P^{(i)}) \\mid \\mid_2^2}_\\text{(1)} - \\underbrace{\\mid \\mid f(A^{(i)}) - f(N^{(i)}) \\mid \\mid_2^2}_\\text{(2)} + \\alpha \\large ] \\small_+ $$\n",
    "\n",
    "Aqui, usa-se a notação \"$[z]_+$\" para definir $max(z,0)$.  \n",
    "\n",
    "deve-se notar que:\n",
    "- O primeiro termo (1) é a distância quadrada entre a âncora \"A\" e o a imagem positiva \"P\" para uma dada tripla; para que as imagens sejam iguais essas distância deve ser pequena. \n",
    "- O segundo termo (2) é a distância quadrada entre uma âncora \"A\" e a imagem negativa \"N\" para uma dada tripla, essa distância deve ser relativamente grande. \n",
    "- $\\alpha$ é a margem. É um hiperparâmetro escolhido manualmente. Usamos $\\alpha = 0.2$. \n",
    "\n",
    "O código a seguir define a função de erro tripla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(y_true, y_pred, alpha = 0.2):\n",
    "    \"\"\"\n",
    "    Implementação da função de erro tripla.\n",
    "    \n",
    "    Argumentos:\n",
    "    y_true -- rótulos true (verdadeiros), necessários quando se define uma perda em Keras, não é necessário nesta função.\n",
    "    y_pred -- lista Python contendo três objetos:\n",
    "            âncora -- as codificações para uma imagem de âncora, com as dimensões (None, 128)\n",
    "            positiva -- as codificações para imagens positivas, com as dimensões (None, 128)\n",
    "            negativa -- as codificações para imagens negativas, com as dimensões (None, 128)\n",
    "    \n",
    "    Returns:\n",
    "    loss -- número real, valor de perda (erro)\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "    \n",
    "    # Computa a distância entre a âncora e a positiva\n",
    "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
    "\n",
    "    # Computa a distância entre a âncora e a positiva\n",
    "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
    "    \n",
    "    # subtrai as duas distãncias anteriores e adiciona o alpha\n",
    "    basic_loss = pos_dist - neg_dist + alpha\n",
    "\n",
    "    # Pega o máximo entre basic_loss e 0.0\n",
    "    loss = tf.reduce_sum(tf.maximum(basic_loss, 0.0))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro = 528.1427\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as test:\n",
    "    tf.set_random_seed(1)\n",
    "    y_true = (None, None, None)\n",
    "    y_pred = (tf.random_normal([3, 128], mean=6, stddev=0.1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=1, stddev=1, seed = 1),\n",
    "              tf.random_normal([3, 128], mean=3, stddev=4, seed = 1))\n",
    "    loss = triplet_loss(y_true, y_pred)\n",
    "    \n",
    "    print(\"Erro = \" + str(loss.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Carregando o modelo de treinado\n",
    "\n",
    "No código seguinte nós carregamos um modelo previamente treinado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRmodel.compile(optimizer = 'adam', loss = triplet_loss, metrics = ['accuracy'])\n",
    "load_weights_from_FaceNet(FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na linha acima, foi carregado um modelo de rede neural que usa o algoritmo de otimização Adam, ele é uma versão melhorada do gradiente descendente, sua implementação é por conta do framework Tensorflow. Também foi passada como parâmetro a função triplet_loss que definimos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Aplicando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um dos problemas que se pode resolver com este modelo, é o da Verificação facial. Por exemplo: uma pessoa com um cartão de acesso tentando passar por uma catraca para ter acesso a algum prédio. A câmera capta a imagem do rosto da pessoa e verifica se essa pessoa que está usando o carão é a pessoa correta, ou seja, sem a necessidade de um ser humano para checar isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Verificação Facial\n",
    "\n",
    "O código a seguir funciona como uma base de dados simples contendo um vetor codificado para cada pessoa com permissão para entrar (passar pela catraca). Para gerar essa cofificação foi usado `img_to_encoding(image_path, model)` que basicamente executa o algoritmo forward propagation do modelo na imagem específica.\n",
    "\n",
    "Nossa base de dados é representada por um dicionário Python. Este dicionário mapea cada nome de pessoa para um vetor codificado de 128 posições, ou seja para cada chave \"nome\" o valor é um vetor codificado de uma imagem.\n",
    "\n",
    "É importante lembrar também que as imagens são todas da mesma dimensão, 96x96 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "database = {}\n",
    "database[\"patterson\"] = img_to_encoding(\"images/patterson.jpg\", FRmodel)\n",
    "database[\"marcelo\"] = img_to_encoding(\"images/marcelo.jpg\", FRmodel)\n",
    "database[\"alexandre\"] = img_to_encoding(\"images/alexandre.jpg\", FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na sequência é implementada a função responsável por verificar se a imagem captada por uma câmera é realmente a pessoa dona do cartão de acesso.Para isso foram implementados os seguintes passos computamos a codificação da imagem de entrada (a imagem captada pela câmera), em seguda calculamos a distância entre essa codificação e a codificação da imagem de identidade (a imagem do dono do cartão de acesso), por último informamos se a pessoa pode ou não passar, isso se a distância for menor que 0.7 (70%), do contrário o acesso é negado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(image_path, identity, database, model):\n",
    "    \"\"\"\n",
    "    Function that verifies if the person on the \"image_path\" image is \"identity\".\n",
    "    \n",
    "    Arguments:\n",
    "    image_path -- path to an image\n",
    "    identity -- string, name of the person you'd like to verify the identity. Has to be a resident of the Happy house.\n",
    "    database -- python dictionary mapping names of allowed people's names (strings) to their encodings (vectors).\n",
    "    model -- your Inception model instance in Keras\n",
    "    \n",
    "    Returns:\n",
    "    dist -- distance between the image_path and the image of \"identity\" in the database.\n",
    "    door_open -- True, if the door should open. False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Computa a codificação da imagem\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Computa a distância entre a imagem da câmera e a imagem da base\n",
    "    dist = np.linalg.norm(encoding - database[identity])\n",
    "    \n",
    "    # Libera o acesso se a distância for menor que 0.7, do contrário nega o acesso. \n",
    "    if dist < 0.7:\n",
    "        print(\"Olá \" + str(identity) + \", bem vindo!\")\n",
    "        door_open = True\n",
    "    else:\n",
    "        print(\"Você não é \" + str(identity) + \", acesso negado.\")\n",
    "        door_open = False\n",
    "        \n",
    "    return dist, door_open"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo, Patterson está tentando passar pela catraca a câmera captura sua imagem (\"images/camera_1.jpg\"). O sistema deve decidir se ele pode ou não passar:\n",
    "\n",
    "<img src=\"images/camera_1.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função verify() criada anteriormente verifica se a imagem captada corresponde ao verdadeiro dono do cartão, ou seja, se é Patterson usando seu cartão de acesso correto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá patterson, bem vindo!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.63389784, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_1.jpg\", \"patterson\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Marcelo perdeu seu cartão de acesso e está tentando entrar com o cartão de outra pessoa. A câmera capta sua imagem e a função de verificação não permite seu acesso. \n",
    "\n",
    "<img src=\"images/marcelo.jpg\" style=\"width:100px;height:100px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Você não é alexandre, acesso negado.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7964067, False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(\"images/camera_1.jpg\", \"alexandre\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Reconhecimento Facial\n",
    "\n",
    "No Reconhecimento Facial queremos que o sistema reconheça uma pessoa apenas capturando uma imagem do seu rosto, para isso é preciso comparar a codificação da imagem capturada com as codificações das imagens das pessoas cadastradas na base. Se uma das imagens tiver uma distância menor ou igual a 0.7, então a pessoa da imagem capturada existe na base e portanto tem acesso liberado.\n",
    "\n",
    "O código a seguir define uma função que faz esse trabalho. Primeiro é computada a codificação da imagem capturada, em seguida aa função procura qual das imagens codificadas da base possui a menor distância comparada à codificação da imagem capturada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def who_is_it(image_path, database, model):\n",
    "    \"\"\"\n",
    "    Implementa o Reconhecimento Facial.\n",
    "    \n",
    "    Argumentos:\n",
    "    image_path -- caminho para a imagem no disco\n",
    "    database -- base de dados contendo codificações de imagem, juntamente com o nome da pessoa na imagem\n",
    "    model -- instancia do modelo de rede neural em Keras\n",
    "    \n",
    "    Returns:\n",
    "    min_dist -- a distância mínima entre a codificação da image_path e as codificações encodings da base\n",
    "    identity -- string, o nome predizido para a pessoa em image_path\n",
    "    \"\"\"\n",
    "    \n",
    "    # Codifica a imagem capturada\n",
    "    encoding = img_to_encoding(image_path, model)\n",
    "    \n",
    "    # Inicializa a menor distância \"min_dist\" com um valor muito grande, 100.\n",
    "    min_dist = 100\n",
    "    \n",
    "    # Encontra a imagem com a codificação mais próxima da imagem capturada\n",
    "    # Itera sobre o dicionário da base de dados obtendo as chaves (names) e as codificações (db_enc).\n",
    "    for (name, db_enc) in database.items():\n",
    "        \n",
    "        # Computa a distância entre a codificação da imagem capturada e a codificação acorrente da base de dados\n",
    "        dist = np.linalg.norm(encoding - db_enc)\n",
    "\n",
    "        # Se a distância for menor que a distÂncia mínima, então passa a ser a nova distância mínima, \n",
    "        # e a identidade passa a ser o nome corrente\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            identity = name\n",
    "    \n",
    "    if min_dist > 0.7:\n",
    "        print(\"Não existe na base de dados.\")\n",
    "    else:\n",
    "        print (\"Olá \" + str(identity) + \", a distância é \" + str(min_dist))\n",
    "        \n",
    "    return min_dist, identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No código abaixo a função who_is_it() verifica se Patterson está na base de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Não existe na base de dados.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9037236, 'patterson')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_is_it(\"images/camera_3.jpg\", database, FRmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "IaknP",
   "launcher_item_id": "5UMr4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
